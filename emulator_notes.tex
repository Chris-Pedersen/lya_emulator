\documentclass[10pt, aps, prd]{revtex4-1}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\newcommand{\Lya}{Lyman-$\alpha\;$}

\newcommand{\spb}[1]{{\textbf{[SPB: #1]}}}
\newcommand{\mnras}{MNRAS}
%opening
% \title{}
% \author{}

% \bibliographystyle{JHEP}

\begin{document}

% \maketitle

% \begin{abstract}
% 
% \end{abstract}

\section{Surrogate Modelling}

We wish to find the confidence regions for a number of parameters using an experiment, in this case measurements of 
the \Lya~forest flux power spectrum. To do this, we are required to evaluate a likelihood function for the data 
given a model and a set of parameters. Our problem is 
that evaluating the predictions of our model for a given choice of parameters is slow, relying on the performance 
of a hydrodynamic cosmological simulation. We thus wish to find a way to choose a small number of parameter values, 
evaluate the model at these values, and interpolate between them in such a way as to achieve the maximum accuracy 
with a given number of simulations. \footnote{This sounds similar to the use-case for Approximate Bayesian 
Computation (ABC), but it is not. ABC is used in cases where evaluation of the model is quick, but evaluation 
of the likelihood is slow. Thus instead of the full likelihood, a lower-dimensional approximation, called a 
summary statistic, is evaluated instead. In our case, the likelihood function is quick to evaluate, but the model is not.}
This is known in the engineering literature as \textit{surrogate modelling} and the interpolation function 
as a \textit{surrogate}\cite{Sacks_1989}. The engineering literature contains several excellent reviews, for example:
\cite{Queipo_2005, forrester2008engineering, forrester2009recent}.

In cosmology, similar techniques are referred to as \textit{emulators}. See, for example, \cite{Heitmann:2009} for the matter power spectrum and 
\cite{Liu:2015, Petri:2015} for weak lensing emulators. Our goal however differs in one important respect from that of these authors; where they 
attempted to construct a function capable of modelling all parameters within a given range accurately, we are only interested in the confidence 
regions of a particular experiment. We shall use this fact to build a surrogate which expends extra model evaluations increasing the 
accuracy of its predictions in the region favoured by the experimental data. We shall refer to the uniformly accurate surrogates found in 
the cosmology literature to date as \textit{emulators}, and our interpolations as \textit{surrogate models}.

Note that our goal also differs from that commonly assumed in the engineering literature, which is generally interested in a single optimal 
design which can be used to, e.g.~build an aircraft wing. Thus we shall generally replace maximum likelihood estimations with Bayesian 
confidence intervals.
% Since the engineers can only approximately realise their design, they should rather search for designs 
% which are also close to optimal in a region around their optimal design, but this is taken care of by their assumption that the 
% response surface is smooth.

The construction of our surrogate model will proceed in the following stages:
\begin{enumerate}
 \item Selection of the initial evaluation points
 \item Construction of a first, possibly inaccurate surrogate and an estimation of the error in the surrogate
 \item First evaluation of the likelihood and parameter confidence limits, marginalised over error in the surrogate
 \item Selection of additional, infill, points at the places where the likelihood error is largest.
 \item Construction of an improved surrogate
 \item Evaluation of an improved likelihood
\end{enumerate}
Steps $4-6$ may be repeated as necessary. \cite{forrester2008engineering} recommends using $1/3$ of the available 
model evaluations on the initial set of points. There is a trade-off between parallelism (the number of 
simulations to run at once) and optimality. In other words, it is probably best to pick each infill simulation at the place 
where the likelihood error is largest in serial. In practice I expect this to make a small difference; probably best is to run as 
many simulations in parallel as we can, subject to having at least two rounds of infill.

As is conventional, in what follows we shall scale each parameter to fit the unit interval and thus the parameter space 
to the unit hypercube; $[0-1]^n$, where $n$ is the number of parameters.

\subsection{Initial Selection of Points}

% Prior to model evaluation, we assume little knowledge of the functional dependence on parameters. 
We wish to choose an initial set of points which has good space-filling properties. To ensure that the surrogate will behave well, 
we also prefer to pick a set of points which are well-separated in parameter space. Random (Monte-Carlo) sampling does not 
possess either of these properties. Instead a Latin Hypercube is commonly used. To ensure even covering, for $k$ samples, 
the $n$-dimensional unit cube is binned (stratified) into $k$ equal-size intervals. For every parameter, exactly one sample 
lies in each bin. This is equivalent to choosing a random permutation of the integers $0-k$. Once the samples are projected to 
a single dimension, they evenly sample the unit interval.

While the natural choice is to place each sample at the centre of the chosen bin, should the underlying model have a component 
which is periodic with a $1/k$ interval, this component will not be modelled by the surrogate. For this reason, the position 
of the sample within the bin is sometimes randomised, allowing periodic components to be picked up at the cost of slightly 
uneven sampling. We do not expect periodic behaviour in our likelihood function, so we do not randomise the locations of 
the samples within the bin.

There are usually a large number of possible Latin hypercubes, some more space-filling than others. %\cite{someone}. 
For example, samples placed along a diagonal are a Latin hypercube, but fill only a one-dimensional subspace of the unit cube. There are a 
variety of ways to avoid this problem. \cite{Heitmann:2009} use \textit{orthogonal latin hypercubes}. This extends the 
one dimensional uniform sampling property of the Latin hypercube to higher dimensions. An orthogonal latin hypercube of rank $r$
has even sampling of each $r$-dimensional subsample. The problem with these designs is that they are inflexible; it becomes difficult
to find solutions satisfying the constraints, especially when using infill based on the likelihood samples, as we will.

Therefore we shall ensure the hypercube is space-filling in a different way; by Monte Carlo sampling Latin hypercubes and 
picking the one with the best space-filling properties. We will maximise the minimum distance between different samples
under the Euclidean norm. Other choices of metric are also possible. It is not necessary to choose the exactly optimally space-filling 
design, merely one which has reasonable good properties. This choice will allow us to extend the number of samples 
and still requiring that the combination form a Latin hypercube.

\subsection{Construction of the Surrogate}

We choose to use a Gaussian Process based surrogate with a covariance function. \cite{Liu:2015} used 
radial basis functions. While these are a good choice, Gaussian Processes provide not just the maximum 
likelihood value for the interpolated function, but a confidence interval for its value, which we 
will use when choosing positions for infill points.

The choice of covariance function is essentially a prior on function space. Since we are able to test 
the accuracy of our emulator fairly easily by leave-one-out cross-validation, we can experiment with different
covariance functions and pick the best one. In practice I suspect it will make little difference.

\subsection{Choice of infill positions}

We wish to choose infill positions where the expected error on the likelihood of the data is largest. This is easy to do with 
gaussian processes because they include an estimate of the error. Note that we implicitly assume that our initial grid 
is fine enough, and the support of the peak broad enough, that our initial sampling picks 
up some hint of the correct global maximum. As long as the likelihood is reasonably Gaussian, this should be true.

\cite{forrester2009recent} suggest a variety of methods for infill, which amount to computing the expected position of the maximum likelihood value 
in unexplored parameter space, using the error estimates from the Gaussian Process. These procedures are all aimed at finding a maximum likelihood 
point (because you want your aircraft wing to have the optimum parameters). We don't want to do any of these things; we are interested in 
estimating the likelihood function. So we can just place new points at the estimated largest error from the gaussian process.

\subsection{Correcting Resolution}

To cover a wider range of scales, it is advantageous to correct between 
and multiply by a correction function based on a small number of high resolution simulations. \cite{McDonald:2005pk} calls this splicing. 
We approximate a simulation with a box size of $L_B$ and a particle number $N_B$ with three smaller simulations. Simulation A has 
box $L_B$ and $N_B/2$ particles. Simulation B has $L_B/2$ and $N_B/4$ particles, and thus the same resolution as simulation A, but a smaller box. 
Any differences between simulations A and B will be due to the smaller box size in simulation B. Finally, simulation C has $L_B/2$ and $N_B$ particles. 
It thus has the same box size as simulation B, but a higher resolution. Any differences between simulation B and simulation C are due to inadequate 
resolution. The result of a $L_B$, $2 N_B$ simulation is approximated by $P_A / P_B * P_C$, where $P_A$ denotes the flux power spectrum of simulation A.
This is also known as Richardson extrapolation \cite{Lukic:2015}.

The difficulty with this method is that for non-linear evolution, modes are correlated in Fourier space. So the growth of the large box modes only modelled 
in simulation A affects growth of small scale modes only modelled in simulation C. Instead there is a technique called co-kriging, where a small number 
of higher resolution simulations are run with the same parameters as a subset of the lower resolution simulations. The difference is described by 
another gaussian process, allowing an estimation of the results that would occur if we had run the whole emulator at higher resolution.

It is not yet clear to me what the balance between high and low resolution simulations should be.
The ideal option is to just use converged simulations, of course, and it may be that this is possible for the 1D power spectrum.

\subsection{Parameters}
Cosmological parameters: $n_s$ and $A_s$. If using knots, this is instead the number of knots.
Thermal parameters: helium photo-heating rate. Increasing this  increases the temperature of the gas. 
Alternatively, we can choose $\gamma$ and $T_0$ directly by implementing a density-dependent rescaling of the heating rate \verb|rescale_gamma|.
We also need $\tau_0$, the mean flux, which we implement in post-processing.
Expansion history: We are simulating only up to $z=2$, which is still fairly deep in the matter era. The expansion rate is
\begin{equation}
 H(z) = H_0 \sqrt{\Omega_m (1+z)^3 + \Omega_\Lambda}
\end{equation}
Fractional uncertainty in the expansion rate due to uncertainty in $\Omega_\Lambda$ (assuming flatness) is:
\begin{equation}
 \delta H / H = \frac{\partial H}{\partial \Omega_\Lambda} \frac{\delta \Omega_\Lambda}{H} = \frac{\delta \Omega_\Lambda /2}{\Omega_m/\Omega_\Lambda (1+z)^3 + 1}
\end{equation}
Planck says $\Omega_m = 0.315 \pm 0.013$. So at $z=2$ we have: $\delta H /H \approx 0.037 \delta \Omega_\Lambda \approx 2\times 10^{-4}$. 
This is small, so we parametrise all uncertainty in the expansion history using $H_0$. Andreu argues that $H_0$ we can parametrise so that
$H_0$ has no effect on the likelihood. In this case, we can still allow for residual uncertainty by varying $\Omega_m$ slightly. This
would be advantageous, because changing $H_0$ too much strictly means one should change the box size as well, which is difficult for 
the emulator.

This gives us four simulation parameters initially: $n_s$, $A_s$, $\gamma$ and $T_0$. 

We could alternatively use knots. For the knot parameters we use four knots, as last time, and space them evenly in log space. 
The power spectrum is multiplied by a piecewise linear polynomial through the specified knots. The boundary conditions are: 
unity on large scales, equal to the smallest-scale knot on small scales. This gives us $6$ parameters.

\subsection{Open difficulties}

We need some estimate of the dataset around which to build our interpolator. It doesn't have to be very good, but some would be nice. This could
be an issue in building something for DESI unless we can use the BOSS 3D power.

\bibliography{surrogate_refs}
\end{document}
